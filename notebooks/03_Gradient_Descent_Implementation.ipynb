{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation\n",
    "\n",
    "This notebook demonstrates gradient descent optimization for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from gradient_descent import GradientDescent\n",
    "from visualization import plot_cost_history, plot_regression_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent Algorithm\n",
    "\n",
    "Update rule: $\\theta := \\theta - \\alpha \\nabla J(\\theta)$\n",
    "\n",
    "Where the gradient is: $\\nabla J(\\theta) = \\frac{1}{m}X^T(X\\theta - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 1)\n",
    "y = 2 * X.flatten() + 1 + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Fit with gradient descent\n",
    "gd_model = GradientDescent(learning_rate=0.1, max_iterations=1000)\n",
    "gd_model.fit(X, y)\n",
    "\n",
    "print(f\"Converged in {gd_model.n_iterations_} iterations\")\n",
    "print(f\"Final cost: {gd_model.cost_history_[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost history\n",
    "plot_cost_history(gd_model.cost_history_)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}